---
title: 'DATA 624: Project 2'
author: "Wei Zhou"
date: "12/11/2020"
output:
  html_document:
    df_print: paged
  word_document:
    fig_height: 5
    fig_width: 7
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      comment = NA, fig.align = "center")
# load(".RData")
```  

# Data Exploration  
```{r load-data-github}
# "Student" soft drink data
ph_data <- read.csv("./project2/StudentData.csv", na.strings = c("", " "), stringsAsFactors = FALSE)
# convert values read as integers to numeric
ph_data[, c(16, 18, 21, 28)] <- sapply(ph_data[, c(16, 18, 21, 28)], as.numeric)
```  

The structure of the data is presented below:  

```{r str}
str(ph_data)
```  

The data contains 2571 observations across 33 variables.  The first variable, `Brand.Code`, is a character; the remaining 32 variables are numeric.  Summary statistics of the variables are presented below:  

```{r summary_table}
library(e1071)
library(pander)
means <- sapply(ph_data[-1], function(y) mean(y, na.rm = TRUE))
medians <- sapply(ph_data[-1], function(y) median(y, na.rm = TRUE))
IQRs <- sapply(ph_data[-1], function(y) IQR(y, na.rm = TRUE))
vars <- sapply(ph_data[-1], function(y) var(y, na.rm = TRUE))
skews <- sapply(ph_data[-1], function(y) skewness(as.numeric(y), na.rm = TRUE))
cors <- as.vector(cor(ph_data$PH, ph_data[,2:ncol(ph_data)], use = "complete.obs"))
NAs <- sapply(ph_data[-1], function(y) sum(length(which(is.na(y)))))
soda_summary <- data.frame(means, medians, IQRs, vars, skews, cors, NAs)
colnames(soda_summary) <- c("MEAN", "MEDIAN", "IQR", "Var", "SKEW", "$r_{PH}$", "NAs")
soda_summary <- round(soda_summary, 2)
pander(soda_summary)
```  

## Missing Values  

As shown in the above summary, there are missing values across the variables -- the frequency and pattern of these missing values are presented below:  

```{r missing-pattern}
library(VIM)
aggr(ph_data, sortVars = TRUE, bar = FALSE, prop = FALSE, gap = 1, cex.axis = 0.7,
     col = c("navyblue", "yellow"), ylab = c("Number Missing", "Pattern"))
```  

The variable `MFR` has over 200 missing values, and the variable `Brand.Code` is missing 120 values.  Due to these high proportions of missingness, observations missing these variables are dropped:  

```{r drop-na}
library(plyr) # loaded for later dependecies to avoid conflicts with dplyr
library(tidyverse)
ph_data <- drop_na(ph_data, MFR, Brand.Code)
```  

Numeric variables are plotted below to determine the best method of imputation:  

```{r var-hist}
theme_set(theme_light())
ph_data %>% 
  select(-Brand.Code) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes(x = Values)) +
  geom_histogram(alpha = 0.25, col = "black", bins = 20) +
  facet_wrap(~ Variable, scales = "free", nrow = 4)
```  

Many of the variables have normal or somewhat-normal distributions, and appear to be continuous variables. Many of the variables show varying levels of skewness, and a few of the skewed distributions follow an almost chi-squared or log-normal distribution. These are predictors (`PSC.Distribution` and `PSC.C02.Distribution`) where transformations should be considered. 

Another interesting pattern are the number of 0 values in the `Hyd.Pressure`(1-3) variables.  The $4^{th}$ `Hyd.Pressure` does not follow this same pattern, so depending on the relationship between these variables and the target and/or other variables, they may be removed outright.  

Some of the predictors appear to be discrete, rather than continuous distributions (`Pressure.Setpoint`, `Alch.Rel`), however, many of these variables are just constrained to a few values, but are still continuous.  Out of all the variables, `Bowl.Setpoint` does seem to be a discrete distribution. A table of all values in the variable is below:    

```{r bwl_stpt_vals}
pander(table(ph_data$Bowl.Setpoint))
```  

```{r bowl_setpt_dist}
ph_data %>% 
  select(Bowl.Setpoint) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes(x = Values)) +
  geom_histogram(alpha = 0.25, col = "black", bins = 20) +
  facet_wrap(~ Variable, scales = "free", nrow = 4)
```  

Boxplots provide an alternate view that allows the spread of the data to be viewed a little more clearly, as well as the presence of outliers and their quantities:  

```{r boxplots}
ph_data %>% 
  select(-Brand.Code) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes(x = 1, y = Values)) +
  geom_boxplot() +
  facet_wrap(~ Variable, scales = "free", nrow = 4)
```  

A number of these variables are so highly skewed, such as `Filler.Speed`, `Oxygen.Filler`, `MFR`, and `Air.Pressurer`, that many of the observations for that predictor are recognized as outliers. This suggests that imputing with the mean will not be accurate; an alternate method should be investigated.  

Before moving on to correlations between predictors, the relationship between the target variable `PH` and the other predictors is visualized using scatterplots:  

```{r scat_plts}
ph_data %>% 
  gather(-PH, -Brand.Code, key="Var", value="Value") %>% 
  ggplot(aes(x=Value, y=PH, color=Brand.Code)) +
  geom_point(alpha=0.6) +
  facet_wrap(~ Var, scales = "free", nrow=4)
```  

Here we see there are no strong patterns (i.e. no defined linear pattern), but there are some relationships between `PH` and the other variables that may be more helpful in predicting the `PH` level of the soft drink.  Predictors like `Alch.Rel`, `Density`, and `Temperature` all appear to have a stronger relationship with `PH` - this makes sense, as these variables have more to do with the make-up of the beverage itself, rather than the filling/bottling process.  

Also interesting is the clustering of the `Brand.Code` within the plots. As correlations are investigated, this predictor does not have a strong relationship with the target, but combined with other predictors, may be more telling.  

## Correlation  

The correlation between the variables is investigated:  

```{r cor}
ph_cors <- cor(ph_data %>% select(-Brand.Code), use="complete.obs")
library(corrplot)
corrplot(as.matrix(ph_cors), method = "color", tl.cex = 0.5, tl.col = "black")
```  

As the columns are organized in the data, some interesting patterns are present in the correlogram. Two areas show distinct positive correlations -- these are the predictors that have something to do with carbonation, and another area where different pressure levels correlate with each other. Another set of variables are negatively correlated with these pressure predictors, these have to do with the filling of the bottles, so this makes sense (`Oxygen.Filler`, `Bowl.Setpoint`, `Pressure.Setpoint`).

Some of these same predictors are also correlated well with the target PH variable. The top positive correlations are below:  

```{r top-ph-cors}
top_ph_cors <- ph_cors %>% 
  as.data.frame() %>% 
  select(Correlation = PH) %>% 
  rownames_to_column("Variable") %>% 
  arrange(desc(Correlation))
top_ph_cors %>%
  top_n(11, Correlation) %>% 
  pander()
```  

The predictors with the highest negative correlation to `PH` are as follows:  
```{r top-neg-ph-cors}
top_ph_cors %>%
  top_n(-10, Correlation) %>%
  arrange(Correlation) %>%
  pander()
```  

###Intercorrelated Predictors  

In addition, some of the variables are highly correlated with each other. The following pairings seem to have something to do with each other in the bottle filling process:  

```{r inter_corr}
library(reshape2)
ph_cors_tri <- ph_cors
ph_cors_tri[lower.tri(ph_cors_tri, diag = TRUE)] <- NA  # prevent duplicates by taking upper triangle of matrix
ph_cors_tri %>% 
  melt() %>%
  as.data.frame() %>%
  filter(!is.na(value)) %>%
  arrange(desc(value)) %>%
  top_n(10) %>%
  pander()
```  

Among the variables with positive correlations with each other, many of them are almost perfectly correlated with one another. We may want to consider removing some of these redundant variables, perhaps the ones that are less correlated to the target variable.  In addition predictors with high negative correlations with each other are below:  

```{r int_cor_neg}
ph_cors_tri %>% 
  melt() %>%
  as.data.frame() %>%
  filter(!is.na(value)) %>%
  arrange(value) %>%
  top_n(-10) %>%
  pander()
```  

## Variable Removal
Three similarly-names variables, `Hyd.Pressure1`, `Hyd.Pressure2`, and `Hyd.Pressure3`show large spikes at values of 0 in the histograms above, and show high correlations.  This suggests that these variables are candidates for removal.  These variables, along with a fourth related variable `Hyd.Pressure4`, are first investigated across the four `Brand.Code` values:  

```{r pressure-brand}
ph_data %>% 
  select(Brand.Code, Hyd.Pressure1:Hyd.Pressure4) %>% 
  gather(HydNum, Value, -Brand.Code) %>% 
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 25) +
  facet_grid(Brand.Code ~ HydNum, scales = "free") +
  theme(panel.grid = element_blank()) +
  scale_x_continuous(NULL, NULL, NULL) +
  scale_y_continuous(NULL, NULL, NULL) +
  ggtitle("Distribution of Hyd.Pressure variables across Brand.Codes")
```  

These distributions suggest that there is no behavior in `Hyd.Pressure1`, `Hyd.Pressure2`, or `Hyd.Pressure3` indicated by `Brand.Code`.  For this reason, these three variables are dropped.  `Hyd.Pressure4` is retained, as there appear to be differences in its behavior across `Brand.Code`:  

```{r drop-pressure}
ph_data <- ph_data %>% select(-(Hyd.Pressure1:Hyd.Pressure3))
```  


# Data Preprocessing
Before imputation, the data is split into predictors and a response:
```{r pred-resp}
ph_pred <- ph_data %>% select(-PH)
ph_resp <- ph_data %>% select(PH)
```


## Imputation
Due to the skewness of and relationship between predictors, they are imputed using k-nearest neighbors.  Prior to this imputation, predictors are centered and scaled to avoid bias in predictive models, and highly-correlated predictors are removed.  The `preProcess` function from the `caret` package is capable of performing all of these operations -- per the documentation for this function:

The operations are applied in this order: zero-variance filter, near-zero variance filter, correlation filter, Box-Cox/Yeo-Johnson/exponential transformation, centering, scaling, range, imputation, PCA, ICA then spatial sign.
```{r preprocess}
library(caret)
# set up pre-processing transformation
ph_preproc <- preProcess(ph_pred, method = c("knnImpute", "center", "scale", "corr"))
# apply pre-processing to data
ph_pred <- predict(ph_preproc, ph_pred)
```


## Partitioning
With pre-processing complete, both predictor and response data are partitioned into a training and testing set:
```{r train-test}
# get rows for training subsets
set.seed(100)  # for replicability
train_rows <- createDataPartition(ph_resp$PH, p = 0.75, list = FALSE)
# create training sets
ph_pred_train <- ph_pred[train_rows, ]
ph_resp_train <- ph_resp[train_rows, ]
# creae test sets
ph_pred_test <- ph_pred[-train_rows, ]
ph_resp_test <- ph_resp[-train_rows, ]
```

For some models used, the character predictor `Brand.Code` needs to be numeric.  Additional data frames (for training & testing) are created with this variable converted:
```{r numeric-df}
ph_pred_train_num <- mutate(ph_pred_train, Brand.Code = as.numeric(as.factor(Brand.Code)))
ph_pred_test_num <- mutate(ph_pred_test, Brand.Code = as.numeric(as.factor(Brand.Code)))
```




# Model Creation
The pre-processed data is used to fit an array of models: linear models; non-linear models; and tree based models

## Linear Models
```{r}
# create single df for fomula based model
regressData <- data.frame(ph_pred_train_num, PH = ph_resp_train, stringsAsFactors = FALSE)

```


### Simple Linear Model
A simple linear model is fit using all of the predictors contained in `ph_pred_train`:
```{r lm, eval = FALSE}
# train model
lmTune <- train(PH ~ ., data = regressData, method = "lm")
# get predictions for test set
lmPred <- predict(lmTune, newdata = ph_pred_test_num)
# evaluate performance of test set
lmPerf <- postResample(pred = lmPred, obs = ph_resp_test)
```

### Generalized Linear Model
Generalized linear models are extended versions of linear models which use a link function to describe the relation of the mean and the linear predictor, and a variance function. GLM can be used to model other distributions, such as binomial, Poisson, and exponential.
```{r glm, eval = FALSE}
# fit model
glmTune <- glm(PH ~ ., data = regressData, family = "quasipoisson")  
# get predictions & performance
glmPred <- predict(glmTune, newdata = ph_pred_test_num)
glmPerf <- postResample(pred = glmPred, obs = ph_resp_test)
```


### Principal Components Regression Model
A principal components regression model seeks to lower the number of coefficients in a model by creating combinations of the variables that explain significant portions of the variance in the data.  Each of the principal components generated are mutually orthogonal, so there is no risk of autocorrelation between predictors.
```{r pcm-tune}
library(pls)
# tune model
pcrTune <- pcr(PH ~ ., data = regressData, scale = TRUE, validation = "CV")
```

The performance of the PCR model is investigated across the number of components:
```{r pcm-perf}
par(mfrow = c(1, 2))
validationplot(pcrTune, val.type = "RMSE")

```

When doing PRC modeling, it is important to keep in mind that the main goal is to see a low cross validation error with a lower number of components than the number of variables in the dataset. In these results, it is clear that the number of components are too high for easy interpretation, resulting in no dimensionality reduction. Based on this, 18 principal components are used for prediction.

```{r pcm-pred, eval = FALSE}
# predict & get performance
pcrPred <- predict(pcrTune, newdata = ph_pred_test_num, ncomp = 18)
pcrPerf <- postResample(pred = pcrPred, ph_resp_test)
```


### Stepwise Backwards Linear Regression Model
The PCM model has too many principal components in this model to be easily interpretable.  As an alternate, stepwise backwards linear regression is conducted:

```{r step-back}
# create full model
full <- lm(PH ~ ., data = regressData)
# step backwards
stepTune <- step(full, data = regressData, direction = "backward", trace = FALSE)
pander(summary(stepTune))
```

The variables chosen for inclusion were `Brand.Code`, `Fill.Ounces`, `PSC.Fill`, `Mnf.Flow`, `Carb.Pressure1`, `Hyd.Pressure4`, `Filler.Level`, `Temperature`, `Usage.cont`, `Carb.Flow`, `Density`, `MFR`, `Pressure.Vacuum`, `Oxygen.Filler`, `Pressure.Setpoint`, and `Carb.Rel`.

Predictions and performance are calculated from the stepwise model:
```{r step-pred, eval = FALSE}
stepPred <- predict(stepTune, newdata = ph_pred_test_num)
stepPerf <- postResample(pred = stepPred, obs = ph_resp_test)
```



## Tree-Based Models
For this project we will see the constraints we encountered with our missing variables and the multiple regression. In the real world, the best model can be only as good as the variables measured by the study. We should also keep in mind that P-values can change based on the specific terms in the model. We can also appreciate that stepwise regression and best subsets regression are great tools and can get you close to the correct model. However, studies have found that they generally don't pick the correct model. With all these complications we agreed that tree-based methods are simple and useful for interpretation, but also Combining many trees can often result in dramatic improvements in prediction accuracy, at the expense of some lost interpretability.  For this project we use 6 models below.

For consistency, the same training controls are used for all tree-based models:
```{r train-control}
# use 15-fold cross-validation for training
set.seed(100)
mdl_ctrl <- trainControl(method = "cv", number = 15)
```


### Conventional Tree Model
Basic regression trees partition the data based on predictors into groups with a relatively homogeneous distribution of the response variable, and predict the mean of the appropriate group for the response.
```{r tree, eval = FALSE}
library(rpart)
# tree of max depth
set.seed(100)
rpartTune <- train(x = ph_pred_train, y = ph_resp_train,
                   method = "rpart2", trControl = mdl_ctrl)
rpartPred <- predict(rpartTune, newdata = ph_pred_test)
rpartPerf <- postResample(pred = rpartPred, obs = ph_resp_test)
```


### Rule-Based Model

### Random Forest Model
While bagging offers a reduction in variance, the trees used are not completely independent of each other, since each tree (and each split of each tree), considers all original predictors.  To de-correlate the trees, random forests select a bootstrapped sample of the data, then randomly select a subset of predictors to partition the trees.  Once these trees have been fit, the predictions of all models in the ensemble are averaged together.
```{r rf, eval = FALSE}
library(randomForest)
set.seed(100)
rfTune <- train(x = ph_pred_train_num, y = ph_resp_train,
                method = "rf", trControl = mdl_ctrl, importance = TRUE)
rfPred <- predict(rfTune, newdata = ph_pred_test_num)
rfPerf <- postResample(pred = rfPred, obs = ph_resp_test)
```


### Boosted Tree Model
Gradient boosted trees function by attempting to reduce the residuals fit by models.  Initially, predictions are simply provided by the mean of the response.  A regression tree is then fit based on the residuals between the actual & predicted response.  This tree is used to predict the residual, and the difference between the previous residual and the predicted residual is added to the previous residual.  This process is repeated for a designated number of iterations, and only the final tree is considered.  This is, however, still an ensemble method, as each model fit is informed by the model before it.
```{r boosted-tree, eval = FALSE}
library(gbm)
set.seed(100)
boostTune <- train(x = ph_pred_train_num, y = ph_resp_train,
                   method = "gbm", trControl = mdl_ctrl, verbose = FALSE)
boostPred <- predict(boostTune, newdata = ph_pred_test_num)
boostPerf <- postResample(pred = boostPred, obs = ph_resp_test)
```


### Cubist Model
Cubist models are an extension of the rule-based model above.  Terminal leaves of trees contain models based on predictors used in previous splits, with intermediate linear models at each step of the tree. Predictions made using terminal leaf's model are "smoothed" by considering predictions from models in previous nodes of the tree. The tree is reduced to a set of rules, which, for simplification, are eliminated via pruning.
```{r cubist, eval = FALSE}
library(Cubist)
set.seed(100)
cubistTune <- train(x = ph_pred_train_num, y = ph_resp_train,
                    method = "cubist", trControl = mdl_ctrl)
cubistPred <- predict(cubistTune, newdata = ph_pred_test_num)
cubistPerf <- postResample(pred = cubistPred, obs = ph_resp_test)
```


# Model Selection & Prediction
## Model Performance Comparison
The resampled RMSE & test set prediction RMSE is shown below for each of the models created:
```{r rmse-perf}
# function to extract & present model performance
model_perf <- function (model_set, metric = "RMSE") {
  mdl_names <- character()
  Resampled <- numeric()
  Test <- numeric()
  for (mdl in model_set) {
    mdl_names <- c(mdl_names, mdl)
    Resampled <- c(Resampled, min(get(paste0(mdl, "Tune"))$results[[metric]]))
    Test <- c(Test, get(paste0(mdl, "Perf"))[metric])
  }
  pander(data.frame(Resampled, Test, row.names = mdl_names),
         digits = 4, caption = paste(metric, "performance for selected models"))
}
# present performance for 15 models created
#model_perf(c("lm", "glm", "pcr", "step",
#             "mars", "nnet", "svmRadial", "svmLinear", "knn",
#             "rpart", "rule", "bag", "rf", "boost", "cubist"))
```

Noting that the models trained using functions other than `caret::train` do not return a resampled RMSE, the best model based on both resampled & test RMSE is the **Cubist model**.  To avoid any bias from looking only at a single metric, the r-squared is also investigated:

```{r rsquared-perf}
#model_perf(c("lm", "glm", "pcr", "step",
#             "mars", "nnet", "svmRadial", "svmLinear", "knn",
#             "rpart", "rule", "bag", "rf", "boost", "cubist"),
#           metric = "Rsquared")
```

The Cubist model also performs best as judged by the r-squared metric on the test set.  While the random forest model performs slightly better on resampled r-squared, it is not by a significant margin.  The Cubist model will be used for prediction.

## Model Investigation
The performance profile of the selected Cubist across different tuning parameters is shown below:
```{r plot-cubist}
# plot(cubistTune, main = "Performance profile of Cubist model")
```

## For this model, the optimal RMSE of is obtained for the model with 5 neighbors & 20 committees.  The 10 most important variables used in this final model are presented below:

```{r cubist-imp}
#cubistImp <- varImp(cubistTune)
#ggplot(cubistImp, top = 10) + ggtitle("Importance of top 10 predictors for cubist model")
```

The model is dominated by `Brand.Code`, with `Mnf.Flow`, `Density`, `Pressure.Vacuum`, and `Oxygen.Filler` registering as the next-most-important variables.  The relationships between `Brand.Code` and the two most important numeric variables, `Mnf.Flow` and `Density`, with `PH` are presented below:
```{r plot-cubist-imp-rel}
data.frame(ph_pred, ph_resp) %>% 
  select(PH, Brand.Code, Mnf.Flow, Density) %>% 
  gather(Variable, Value, -c(PH, Brand.Code)) %>% 
  ggplot(aes(x = Value, y = PH, col = Brand.Code)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ Variable, scales = "free_x") +
  theme(legend.position = "bottom") +
  labs(title = "Relationship of important variables to PH", x = NULL)
```

There is not an immediately apparent relationship between `Mnf.flow` and `PH` nor `Brand.Code`, but the relationship between `Density` and `Brand.Code` shows fairly clear clustering behavior.  Within each cluster, there appears to be a weakly positive relationship with `PH`.


## Prediction of Additional Data
The Cubist model is finally used to predict additional data not part of the training or test set.
```{r eval-data}
# read in data
new_data <- read.csv("project2/StudentEvaluation.csv", na.strings = c("", " "), stringsAsFactors = FALSE)
```

The new data is transformed in the same way as the original data:
```{r trans-new}
# remove ph to get only predictors
new_pred <- new_data %>% select(-PH)
# remove Hyd.Pressure1, Hyd.Pressure2, & Hyd.Pressure3
new_pred <- new_pred %>% select(-(Hyd.Pressure1:Hyd.Pressure3))
# perform transformations used for original data
new_pred <- predict(ph_preproc, new_pred)
# convert Brand.Code to numeric
new_pred <- mutate(new_pred, Brand.Code = as.numeric(as.factor(Brand.Code)))
```

Finally, the response is predicted and output:
```{r predict-eval}
# predict
cubistTune <- train(x = ph_pred_train_num, y = ph_resp_train,
                    method = "cubist", trControl = mdl_ctrl)
new_resp <- predict(cubistTune, new_pred)
# write predictors & response to csv
# data.frame(new_data, PH = new_resp) %>% 
#   write_csv("Data624_Project2_prediction.csv")
```

The predicted data is included alongside this submission as *Data624_Project2_prediction.csv*.